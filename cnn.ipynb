{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37949, 11)\n",
      "Index(['Index', 'Author', 'Date published', 'Category', 'Section', 'Url',\n",
      "       'Headline', 'Description', 'Keywords', 'Second headline',\n",
      "       'Article text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = \"cnn2011/CNN_Articels_clean_2/CNN_Articels_clean.csv\"\n",
    "with open(path) as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_person(author):\n",
    "    return not (\n",
    "        len(author.split()) < 2\n",
    "        or re.search(r\"\\b(Staff|Reuters)\\b\", author, re.IGNORECASE)\n",
    "        or \" and \" in author\n",
    "        or author == \"By \"\n",
    "    )\n",
    "\n",
    "\n",
    "# drops if: article text or author is missing, author is not a single person, or article text is too short\n",
    "df = df.dropna(subset=[\"Article text\", \"Author\"])\n",
    "df = df[df[\"Author\"].map(is_single_person)]\n",
    "df = df[df[\"Article text\"].str.len() > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Article text length\"] = df[\"Article text\"].apply(len)\n",
    "author_article_length = (\n",
    "    df.groupby(\"Author\")[\"Article text length\"].sum().sort_values(ascending=False)\n",
    ")\n",
    "df = df[df[\"Author\"].isin(author_article_length[author_article_length > 1000000].index)]\n",
    "\n",
    "# RANDOMIZATION OF THE DATA\n",
    "# this allows us to split the data using indices\n",
    "df = df.sample(frac=1, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topauthors = df.groupby(\"Author\")[\"Article text\"].apply(\" \".join)\n",
    "topauthors = topauthors.str.slice(0, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 word ~ 5 characters\n",
    "charcount = 10000\n",
    "\n",
    "\n",
    "def split_text(text, charcount):\n",
    "    text = text[: len(text) - (len(text) % charcount)]\n",
    "    return [text[i : i + charcount] for i in range(0, len(text), charcount)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'be', 'are', 'been', 'is', 's', 'was', 'were', 'of', 'and', 'to', 'a', 'an', 'in', 'have', 'had', 'has', 'it', 'he', 'his', 'that', 'i', 'for', 'they', 'their', 'you', 'not', 't', 'on', 'she', 'her', 'with', 'as', 'this', 'we', 'at', 'by', 'but', 'from', 'or', 'which', 'will', 'there', 'no', 'can', 'if', 'what', 'would', 'so', 'up']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open(\"vocab.txt\", \"r\") as file:\n",
    "    vocab = file.read().splitlines()\n",
    "print(vocab)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=vocab, stop_words=None, token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ai_df = pd.read_csv(\"rewrites.csv\")\n",
    "ai_train_df, ai_test_df = train_test_split(ai_df, test_size=0.5, random_state=40)\n",
    "\n",
    "ai_train = \" \".join(ai_train_df[\"Generated text\"])\n",
    "ai_test = \" \".join(ai_test_df[\"Generated text\"])\n",
    "\n",
    "ai_train_subsets = split_text(ai_train, charcount)\n",
    "ai_test_subsets = split_text(ai_test, charcount)\n",
    "\n",
    "ai_train_subsets_df = pd.DataFrame(ai_train_subsets, columns=[\"Text chunk\"])\n",
    "ai_test_subsets_df = pd.DataFrame(ai_test_subsets, columns=[\"Text chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "\n",
    "ai_train_subsets_df[\"Count\"] = list(\n",
    "    vectorizer.transform(ai_train_subsets_df[\"Text chunk\"]).toarray()\n",
    ")\n",
    "ai_train_subsets_df[\"Proportion\"] = ai_train_subsets_df[\"Count\"].apply(\n",
    "    lambda x: x / x.sum() if x.sum() != 0 else x\n",
    ")\n",
    "\n",
    "ai_fit_params = []\n",
    "for i, feature in enumerate(feature_names):\n",
    "    aidist = ai_train_subsets_df[\"Proportion\"].apply(lambda x: x[i])\n",
    "    ai_fit_params.append(skewnorm.fit(aidist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(x, params):\n",
    "    a, loc, scale = params\n",
    "    p_value = skewnorm.cdf(x, a=a, loc=loc, scale=scale)\n",
    "    p_value = 2 - 2 * p_value if p_value > 0.5 else 2 * p_value\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def p_value(vector, fits):\n",
    "    p_values = []\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        params = fits[i]\n",
    "        p_values.append(get_p(vector[i], params))\n",
    "\n",
    "    return np.mean(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_subsets, inverse=False, ai_bias=1):\n",
    "    test_subsets_df = pd.DataFrame(test_subsets, columns=[\"Text chunk\"])\n",
    "    test_subsets_df[\"Count\"] = list(\n",
    "        vectorizer.transform(test_subsets_df[\"Text chunk\"]).toarray()\n",
    "    )\n",
    "    test_subsets_df[\"Proportion\"] = test_subsets_df[\"Count\"].apply(\n",
    "        lambda x: x / x.sum() if x.sum() != 0 else x\n",
    "    )\n",
    "\n",
    "    test_subsets_df[\"Vectors\"] = list(\n",
    "        vectorizer.transform(test_subsets_df[\"Text chunk\"]).toarray()\n",
    "    )\n",
    "    test_subsets_df[\"Vectors\"] = test_subsets_df[\"Vectors\"].apply(\n",
    "        lambda x: x / x.sum() if x.sum() != 0 else x\n",
    "    )\n",
    "\n",
    "    test_subsets_df[\"p_human\"] = test_subsets_df[\"Vectors\"].apply(\n",
    "        lambda x: p_value(x, human_fit_params)\n",
    "    )\n",
    "    test_subsets_df[\"p_ai\"] = test_subsets_df[\"Vectors\"].apply(\n",
    "        lambda x: p_value(x, ai_fit_params)\n",
    "    )\n",
    "\n",
    "    test_subsets_df[\"Prediction\"] = test_subsets_df[\"p_human\"] > ai_bias * test_subsets_df[\"p_ai\"]\n",
    "    if inverse:\n",
    "        test_subsets_df[\"Prediction\"] = ~test_subsets_df[\"Prediction\"]\n",
    "\n",
    "    return test_subsets_df[\"Prediction\"].sum(), len(test_subsets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_correct, human_total, ai_correct, ai_total = 0, 0, 0, 0\n",
    "\n",
    "for author, text in topauthors.items():\n",
    "    i_split = int(len(text) * 0.5)\n",
    "    train, test = text[:i_split], text[i_split:]\n",
    "    human_train_subsets_df, human_test_subsets_df = (\n",
    "        pd.DataFrame(split_text(train, charcount), columns=[\"Text chunk\"]),\n",
    "        pd.DataFrame(split_text(test, charcount), columns=[\"Text chunk\"]),\n",
    "    )\n",
    "\n",
    "    human_train_subsets_df[\"Count\"] = list(\n",
    "        vectorizer.transform(human_train_subsets_df[\"Text chunk\"]).toarray()\n",
    "    )\n",
    "    human_train_subsets_df[\"Proportion\"] = human_train_subsets_df[\"Count\"].apply(\n",
    "        lambda x: x / x.sum() if x.sum() != 0 else x\n",
    "    )\n",
    "\n",
    "    human_fit_params = []\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        humandist = human_train_subsets_df[\"Proportion\"].apply(lambda x: x[i])\n",
    "        human_fit_params.append(skewnorm.fit(humandist))\n",
    "\n",
    "    ai_bias = 1.2\n",
    "\n",
    "    human_results = classify(human_test_subsets_df, ai_bias=ai_bias)\n",
    "    human_correct += human_results[0]\n",
    "    human_total += human_results[1]\n",
    "\n",
    "    ai_results = classify(ai_test_subsets_df, inverse=True, ai_bias=ai_bias)\n",
    "    ai_correct += ai_results[0]\n",
    "    ai_total += ai_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on humans: 0.9714285714285714 (1020/1050)\n",
      "Accuracy on AI: 0.9927113702623906 (6129/6174)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on humans:\", human_correct / human_total, f\"({human_correct}/{human_total})\")\n",
    "print(\"Accuracy on AI:\", ai_correct / ai_total, f\"({ai_correct}/{ai_total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9951290793960058\n",
      "Recall: 0.9927113702623906\n",
      "F1: 0.993918754560934\n",
      "F0.5: 0.9946445959104189\n"
     ]
    }
   ],
   "source": [
    "def f_beta(precision, recall, beta=1):\n",
    "    return (1 + beta ** 2) * (precision * recall) / ((beta ** 2 * precision) + recall)\n",
    "\n",
    "tp = ai_correct\n",
    "fp = human_total - human_correct\n",
    "fn = ai_total - ai_correct\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = f_beta(precision, recall)\n",
    "f05 = f_beta(precision, recall, beta=0.5)\n",
    "\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1: {f1}\\nF0.5: {f05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6129 30 45\n"
     ]
    }
   ],
   "source": [
    "print(tp, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
